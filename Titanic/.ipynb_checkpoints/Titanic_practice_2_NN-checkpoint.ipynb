{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"mod_train.csv\")\n",
    "test=pd.read_csv(\"mod_test.csv\")\n",
    "test_result=pd.read_csv(\"mod_test_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train.drop(['Survived', 'Sex','Age', 'Fare', 'Embarked', 'Cabin', 'Unnamed: 0'], axis=1)\n",
    "y_train=train['Survived']\n",
    "\n",
    "x_val=x_train[800:].values\n",
    "y_val=y_train[800:].values\n",
    "x_train=x_train[:800].values\n",
    "y_train=y_train[:800].values\n",
    "\n",
    "x_test=test.drop(['Sex', 'Fare', 'Age','Embarked', 'Cabin', 'Unnamed: 0'], axis=1).values\n",
    "y_test=test_result.drop(['Unnamed: 0'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 7), (800,), (418, 7))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reluModel64_3():    \n",
    "    model=models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(7,)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reluModel64_5():\n",
    "    model=models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(7,)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model64_3=reluModel64_3()\n",
    "model64_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1=model64_3.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hist1_dict=hist1.history\n",
    "loss=hist1_dict['loss']\n",
    "val_loss=hist1_dict['val_loss']\n",
    "\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\conda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\conda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\conda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\conda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\conda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\conda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 13,057\n",
      "Trainable params: 13,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\conda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 800 samples, validate on 91 samples\n",
      "Epoch 1/300\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.6673 - acc: 0.6325 - val_loss: 0.5758 - val_acc: 0.7363\n",
      "Epoch 2/300\n",
      "800/800 [==============================] - 0s 104us/step - loss: 0.5963 - acc: 0.7188 - val_loss: 0.5214 - val_acc: 0.7582\n",
      "Epoch 3/300\n",
      "800/800 [==============================] - 0s 96us/step - loss: 0.5749 - acc: 0.7263 - val_loss: 0.4911 - val_acc: 0.8681\n",
      "Epoch 4/300\n",
      "800/800 [==============================] - 0s 98us/step - loss: 0.5442 - acc: 0.7400 - val_loss: 0.4891 - val_acc: 0.7473\n",
      "Epoch 5/300\n",
      "800/800 [==============================] - 0s 86us/step - loss: 0.5340 - acc: 0.7488 - val_loss: 0.4354 - val_acc: 0.8571\n",
      "Epoch 6/300\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.4803 - acc: 0.7800 - val_loss: 0.4129 - val_acc: 0.8791\n",
      "Epoch 7/300\n",
      "800/800 [==============================] - 0s 85us/step - loss: 0.4585 - acc: 0.8038 - val_loss: 0.5547 - val_acc: 0.7692\n",
      "Epoch 8/300\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.4966 - acc: 0.7812 - val_loss: 0.3966 - val_acc: 0.8571\n",
      "Epoch 9/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.4563 - acc: 0.8175 - val_loss: 0.3984 - val_acc: 0.8462\n",
      "Epoch 10/300\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.4243 - acc: 0.8313 - val_loss: 0.3911 - val_acc: 0.8571\n",
      "Epoch 11/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.4116 - acc: 0.8287 - val_loss: 0.3865 - val_acc: 0.8571\n",
      "Epoch 12/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.4221 - acc: 0.8313 - val_loss: 0.3938 - val_acc: 0.8352\n",
      "Epoch 13/300\n",
      "800/800 [==============================] - 0s 104us/step - loss: 0.4073 - acc: 0.8313 - val_loss: 0.4034 - val_acc: 0.8571\n",
      "Epoch 14/300\n",
      "800/800 [==============================] - 0s 105us/step - loss: 0.4050 - acc: 0.8362 - val_loss: 0.4013 - val_acc: 0.8352\n",
      "Epoch 15/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.4236 - acc: 0.8200 - val_loss: 0.4481 - val_acc: 0.8352\n",
      "Epoch 16/300\n",
      "800/800 [==============================] - 0s 99us/step - loss: 0.4115 - acc: 0.8362 - val_loss: 0.4157 - val_acc: 0.8462\n",
      "Epoch 17/300\n",
      "800/800 [==============================] - 0s 102us/step - loss: 0.4172 - acc: 0.8237 - val_loss: 0.4197 - val_acc: 0.8681\n",
      "Epoch 18/300\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.4294 - acc: 0.8188 - val_loss: 0.4200 - val_acc: 0.8571\n",
      "Epoch 19/300\n",
      "800/800 [==============================] - 0s 99us/step - loss: 0.4041 - acc: 0.8362 - val_loss: 0.4170 - val_acc: 0.8571\n",
      "Epoch 20/300\n",
      "800/800 [==============================] - 0s 98us/step - loss: 0.4331 - acc: 0.8188 - val_loss: 0.4339 - val_acc: 0.8571\n",
      "Epoch 21/300\n",
      "800/800 [==============================] - 0s 96us/step - loss: 0.4022 - acc: 0.8325 - val_loss: 0.4378 - val_acc: 0.8352\n",
      "Epoch 22/300\n",
      "800/800 [==============================] - 0s 99us/step - loss: 0.4133 - acc: 0.8137 - val_loss: 0.4910 - val_acc: 0.8462\n",
      "Epoch 23/300\n",
      "800/800 [==============================] - 0s 106us/step - loss: 0.4090 - acc: 0.8325 - val_loss: 0.4461 - val_acc: 0.8571\n",
      "Epoch 24/300\n",
      "800/800 [==============================] - 0s 90us/step - loss: 0.3991 - acc: 0.8313 - val_loss: 0.4657 - val_acc: 0.8352\n",
      "Epoch 25/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.4090 - acc: 0.8237 - val_loss: 0.4679 - val_acc: 0.8242\n",
      "Epoch 26/300\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.3973 - acc: 0.8400 - val_loss: 0.4726 - val_acc: 0.8681\n",
      "Epoch 27/300\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.3922 - acc: 0.8450 - val_loss: 0.4780 - val_acc: 0.8242\n",
      "Epoch 28/300\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.3970 - acc: 0.8387 - val_loss: 0.4775 - val_acc: 0.8242\n",
      "Epoch 29/300\n",
      "800/800 [==============================] - 0s 106us/step - loss: 0.3838 - acc: 0.8387 - val_loss: 0.5112 - val_acc: 0.8352\n",
      "Epoch 30/300\n",
      "800/800 [==============================] - 0s 114us/step - loss: 0.3829 - acc: 0.8362 - val_loss: 0.4942 - val_acc: 0.8462\n",
      "Epoch 31/300\n",
      "800/800 [==============================] - 0s 102us/step - loss: 0.3814 - acc: 0.8425 - val_loss: 0.5031 - val_acc: 0.8571\n",
      "Epoch 32/300\n",
      "800/800 [==============================] - 0s 114us/step - loss: 0.3896 - acc: 0.8463 - val_loss: 0.5197 - val_acc: 0.8242\n",
      "Epoch 33/300\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.3900 - acc: 0.8438 - val_loss: 0.5224 - val_acc: 0.8681\n",
      "Epoch 34/300\n",
      "800/800 [==============================] - 0s 91us/step - loss: 0.3832 - acc: 0.8475 - val_loss: 0.5244 - val_acc: 0.8132\n",
      "Epoch 35/300\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.3788 - acc: 0.8438 - val_loss: 0.5151 - val_acc: 0.8462\n",
      "Epoch 36/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3850 - acc: 0.8425 - val_loss: 0.5251 - val_acc: 0.8462\n",
      "Epoch 37/300\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.3914 - acc: 0.8325 - val_loss: 0.5156 - val_acc: 0.8681\n",
      "Epoch 38/300\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.3992 - acc: 0.8250 - val_loss: 0.5257 - val_acc: 0.8681\n",
      "Epoch 39/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.3835 - acc: 0.8412 - val_loss: 0.5169 - val_acc: 0.8571\n",
      "Epoch 40/300\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.3796 - acc: 0.8400 - val_loss: 0.5172 - val_acc: 0.8242\n",
      "Epoch 41/300\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.3775 - acc: 0.8450 - val_loss: 0.5170 - val_acc: 0.8242\n",
      "Epoch 42/300\n",
      "800/800 [==============================] - 0s 99us/step - loss: 0.3850 - acc: 0.8450 - val_loss: 0.5194 - val_acc: 0.8242\n",
      "Epoch 43/300\n",
      "800/800 [==============================] - 0s 98us/step - loss: 0.3754 - acc: 0.8387 - val_loss: 0.5196 - val_acc: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300\n",
      "800/800 [==============================] - 0s 100us/step - loss: 0.3969 - acc: 0.8313 - val_loss: 0.5194 - val_acc: 0.8242\n",
      "Epoch 45/300\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.3819 - acc: 0.8412 - val_loss: 0.5236 - val_acc: 0.8571\n",
      "Epoch 46/300\n",
      "800/800 [==============================] - 0s 90us/step - loss: 0.3867 - acc: 0.8350 - val_loss: 0.5187 - val_acc: 0.8242\n",
      "Epoch 47/300\n",
      "800/800 [==============================] - 0s 83us/step - loss: 0.3766 - acc: 0.8387 - val_loss: 0.5211 - val_acc: 0.8571\n",
      "Epoch 48/300\n",
      "800/800 [==============================] - 0s 85us/step - loss: 0.3797 - acc: 0.8463 - val_loss: 0.5768 - val_acc: 0.8352\n",
      "Epoch 49/300\n",
      "800/800 [==============================] - 0s 85us/step - loss: 0.3912 - acc: 0.8287 - val_loss: 0.5211 - val_acc: 0.8242\n",
      "Epoch 50/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3712 - acc: 0.8488 - val_loss: 0.5246 - val_acc: 0.8242\n",
      "Epoch 51/300\n",
      "800/800 [==============================] - 0s 88us/step - loss: 0.3673 - acc: 0.8438 - val_loss: 0.5415 - val_acc: 0.8352\n",
      "Epoch 52/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.3780 - acc: 0.8400 - val_loss: 0.5333 - val_acc: 0.8242\n",
      "Epoch 53/300\n",
      "800/800 [==============================] - 0s 90us/step - loss: 0.3646 - acc: 0.8463 - val_loss: 0.5332 - val_acc: 0.8462\n",
      "Epoch 54/300\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.3726 - acc: 0.8450 - val_loss: 0.5270 - val_acc: 0.8571\n",
      "Epoch 55/300\n",
      "800/800 [==============================] - 0s 91us/step - loss: 0.3651 - acc: 0.8525 - val_loss: 0.5476 - val_acc: 0.8352\n",
      "Epoch 56/300\n",
      "800/800 [==============================] - 0s 100us/step - loss: 0.3775 - acc: 0.8450 - val_loss: 0.5404 - val_acc: 0.8352\n",
      "Epoch 57/300\n",
      "800/800 [==============================] - 0s 103us/step - loss: 0.3724 - acc: 0.8488 - val_loss: 0.5648 - val_acc: 0.8352\n",
      "Epoch 58/300\n",
      "800/800 [==============================] - 0s 88us/step - loss: 0.3908 - acc: 0.8325 - val_loss: 0.5471 - val_acc: 0.8571\n",
      "Epoch 59/300\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.3805 - acc: 0.8362 - val_loss: 0.5393 - val_acc: 0.8352\n",
      "Epoch 60/300\n",
      "800/800 [==============================] - 0s 98us/step - loss: 0.3656 - acc: 0.8425 - val_loss: 0.5315 - val_acc: 0.8352\n",
      "Epoch 61/300\n",
      "800/800 [==============================] - 0s 88us/step - loss: 0.3803 - acc: 0.8425 - val_loss: 0.5366 - val_acc: 0.8242\n",
      "Epoch 62/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.3706 - acc: 0.8475 - val_loss: 0.5344 - val_acc: 0.8132\n",
      "Epoch 63/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3651 - acc: 0.8463 - val_loss: 0.5298 - val_acc: 0.8571\n",
      "Epoch 64/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.3662 - acc: 0.8562 - val_loss: 0.5461 - val_acc: 0.8352\n",
      "Epoch 65/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.3682 - acc: 0.8450 - val_loss: 0.5450 - val_acc: 0.8462\n",
      "Epoch 66/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.3656 - acc: 0.8475 - val_loss: 0.5299 - val_acc: 0.8242\n",
      "Epoch 67/300\n",
      "800/800 [==============================] - 0s 88us/step - loss: 0.3662 - acc: 0.8450 - val_loss: 0.5462 - val_acc: 0.8242\n",
      "Epoch 68/300\n",
      "800/800 [==============================] - 0s 86us/step - loss: 0.3642 - acc: 0.8500 - val_loss: 0.5513 - val_acc: 0.8571\n",
      "Epoch 69/300\n",
      "800/800 [==============================] - 0s 83us/step - loss: 0.3758 - acc: 0.8475 - val_loss: 0.5299 - val_acc: 0.8242\n",
      "Epoch 70/300\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.3714 - acc: 0.8475 - val_loss: 0.5418 - val_acc: 0.8242\n",
      "Epoch 71/300\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.3617 - acc: 0.8538 - val_loss: 0.5450 - val_acc: 0.8242\n",
      "Epoch 72/300\n",
      "800/800 [==============================] - 0s 100us/step - loss: 0.3725 - acc: 0.8500 - val_loss: 0.5393 - val_acc: 0.8571\n",
      "Epoch 73/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3714 - acc: 0.8475 - val_loss: 0.5498 - val_acc: 0.8462\n",
      "Epoch 74/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3657 - acc: 0.8562 - val_loss: 0.5439 - val_acc: 0.8462\n",
      "Epoch 75/300\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.3617 - acc: 0.8538 - val_loss: 0.5577 - val_acc: 0.8462\n",
      "Epoch 76/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.3571 - acc: 0.8550 - val_loss: 0.5590 - val_acc: 0.8132\n",
      "Epoch 77/300\n",
      "800/800 [==============================] - 0s 96us/step - loss: 0.3842 - acc: 0.8337 - val_loss: 0.5588 - val_acc: 0.8571\n",
      "Epoch 78/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.3899 - acc: 0.8488 - val_loss: 0.5505 - val_acc: 0.8462\n",
      "Epoch 79/300\n",
      "800/800 [==============================] - 0s 86us/step - loss: 0.3662 - acc: 0.8500 - val_loss: 0.5558 - val_acc: 0.8352\n",
      "Epoch 80/300\n",
      "800/800 [==============================] - 0s 105us/step - loss: 0.3661 - acc: 0.8588 - val_loss: 0.5489 - val_acc: 0.8462\n",
      "Epoch 81/300\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.3630 - acc: 0.8513 - val_loss: 0.5564 - val_acc: 0.8242\n",
      "Epoch 82/300\n",
      "800/800 [==============================] - 0s 100us/step - loss: 0.3676 - acc: 0.8513 - val_loss: 0.5674 - val_acc: 0.8132\n",
      "Epoch 83/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.3795 - acc: 0.8412 - val_loss: 0.5520 - val_acc: 0.8242\n",
      "Epoch 84/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3593 - acc: 0.8525 - val_loss: 0.5588 - val_acc: 0.8352\n",
      "Epoch 85/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.3602 - acc: 0.8575 - val_loss: 0.5663 - val_acc: 0.8571\n",
      "Epoch 86/300\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.3625 - acc: 0.8538 - val_loss: 0.5607 - val_acc: 0.8242\n",
      "Epoch 87/300\n",
      "800/800 [==============================] - 0s 108us/step - loss: 0.3620 - acc: 0.8525 - val_loss: 0.5721 - val_acc: 0.8132\n",
      "Epoch 88/300\n",
      "800/800 [==============================] - 0s 100us/step - loss: 0.3561 - acc: 0.8600 - val_loss: 0.5548 - val_acc: 0.8352\n",
      "Epoch 89/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3630 - acc: 0.8525 - val_loss: 0.5763 - val_acc: 0.8242\n",
      "Epoch 90/300\n",
      "800/800 [==============================] - 0s 96us/step - loss: 0.3487 - acc: 0.8562 - val_loss: 0.5542 - val_acc: 0.8352\n",
      "Epoch 91/300\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.3614 - acc: 0.8525 - val_loss: 0.6110 - val_acc: 0.8022\n",
      "Epoch 92/300\n",
      "800/800 [==============================] - 0s 86us/step - loss: 0.3668 - acc: 0.8513 - val_loss: 0.5823 - val_acc: 0.8022\n",
      "Epoch 93/300\n",
      "800/800 [==============================] - 0s 102us/step - loss: 0.3531 - acc: 0.8538 - val_loss: 0.5807 - val_acc: 0.8571\n",
      "Epoch 94/300\n",
      "800/800 [==============================] - 0s 96us/step - loss: 0.3540 - acc: 0.8588 - val_loss: 0.5929 - val_acc: 0.8352\n",
      "Epoch 95/300\n",
      "800/800 [==============================] - 0s 103us/step - loss: 0.3627 - acc: 0.8550 - val_loss: 0.5862 - val_acc: 0.8462\n",
      "Epoch 96/300\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.3549 - acc: 0.8562 - val_loss: 0.5954 - val_acc: 0.7912\n",
      "Epoch 97/300\n",
      "800/800 [==============================] - 0s 81us/step - loss: 0.3493 - acc: 0.8538 - val_loss: 0.5834 - val_acc: 0.8242\n",
      "Epoch 98/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.3492 - acc: 0.8588 - val_loss: 0.5878 - val_acc: 0.8352\n",
      "Epoch 99/300\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.3441 - acc: 0.8562 - val_loss: 0.5866 - val_acc: 0.8132\n",
      "Epoch 100/300\n",
      "800/800 [==============================] - 0s 104us/step - loss: 0.3433 - acc: 0.8588 - val_loss: 0.6038 - val_acc: 0.8571\n",
      "Epoch 101/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3535 - acc: 0.8550 - val_loss: 0.5795 - val_acc: 0.8352\n",
      "Epoch 102/300\n",
      "800/800 [==============================] - 0s 100us/step - loss: 0.3465 - acc: 0.8550 - val_loss: 0.5928 - val_acc: 0.8132\n",
      "Epoch 103/300\n",
      "800/800 [==============================] - 0s 106us/step - loss: 0.3492 - acc: 0.8550 - val_loss: 0.5918 - val_acc: 0.8132\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 101us/step - loss: 0.3485 - acc: 0.8612 - val_loss: 0.5888 - val_acc: 0.8352\n",
      "Epoch 105/300\n",
      "800/800 [==============================] - 0s 97us/step - loss: 0.3563 - acc: 0.8562 - val_loss: 0.6004 - val_acc: 0.8571\n",
      "Epoch 106/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.3574 - acc: 0.8550 - val_loss: 0.5860 - val_acc: 0.8462\n",
      "Epoch 107/300\n",
      "800/800 [==============================] - 0s 90us/step - loss: 0.3562 - acc: 0.8500 - val_loss: 0.6153 - val_acc: 0.8022\n",
      "Epoch 108/300\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.3583 - acc: 0.8538 - val_loss: 0.6143 - val_acc: 0.8462\n",
      "Epoch 109/300\n",
      "800/800 [==============================] - 0s 85us/step - loss: 0.3496 - acc: 0.8562 - val_loss: 0.5951 - val_acc: 0.8352\n",
      "Epoch 110/300\n",
      "800/800 [==============================] - 0s 98us/step - loss: 0.3438 - acc: 0.8650 - val_loss: 0.6097 - val_acc: 0.8132\n",
      "Epoch 111/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3494 - acc: 0.8538 - val_loss: 0.6107 - val_acc: 0.8022\n",
      "Epoch 112/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.3456 - acc: 0.8612 - val_loss: 0.6060 - val_acc: 0.8132\n",
      "Epoch 113/300\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.3500 - acc: 0.8550 - val_loss: 0.6282 - val_acc: 0.8022\n",
      "Epoch 114/300\n",
      "800/800 [==============================] - 0s 105us/step - loss: 0.3651 - acc: 0.8588 - val_loss: 0.6030 - val_acc: 0.8352\n",
      "Epoch 115/300\n",
      "800/800 [==============================] - 0s 96us/step - loss: 0.3458 - acc: 0.8663 - val_loss: 0.6015 - val_acc: 0.8242\n",
      "Epoch 116/300\n",
      "800/800 [==============================] - 0s 90us/step - loss: 0.3509 - acc: 0.8575 - val_loss: 0.6201 - val_acc: 0.8132\n",
      "Epoch 117/300\n",
      "800/800 [==============================] - 0s 99us/step - loss: 0.3457 - acc: 0.8738 - val_loss: 0.6337 - val_acc: 0.8022\n",
      "Epoch 118/300\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.3467 - acc: 0.8638 - val_loss: 0.6460 - val_acc: 0.7912\n",
      "Epoch 119/300\n",
      "800/800 [==============================] - 0s 104us/step - loss: 0.3484 - acc: 0.8513 - val_loss: 0.6133 - val_acc: 0.8132\n",
      "Epoch 120/300\n",
      "800/800 [==============================] - 0s 90us/step - loss: 0.3388 - acc: 0.8663 - val_loss: 0.6148 - val_acc: 0.8022\n",
      "Epoch 121/300\n",
      "800/800 [==============================] - 0s 83us/step - loss: 0.3391 - acc: 0.8625 - val_loss: 0.6280 - val_acc: 0.8022\n",
      "Epoch 122/300\n",
      "800/800 [==============================] - 0s 99us/step - loss: 0.3438 - acc: 0.8625 - val_loss: 0.6273 - val_acc: 0.8022\n",
      "Epoch 123/300\n",
      "800/800 [==============================] - 0s 104us/step - loss: 0.3455 - acc: 0.8575 - val_loss: 0.6483 - val_acc: 0.8022\n",
      "Epoch 124/300\n",
      "800/800 [==============================] - 0s 104us/step - loss: 0.3405 - acc: 0.8575 - val_loss: 0.6369 - val_acc: 0.7912\n",
      "Epoch 125/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.3362 - acc: 0.8625 - val_loss: 0.6549 - val_acc: 0.8022\n",
      "Epoch 126/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.3405 - acc: 0.8575 - val_loss: 0.6471 - val_acc: 0.8352\n",
      "Epoch 127/300\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.3380 - acc: 0.8612 - val_loss: 0.6283 - val_acc: 0.8132\n",
      "Epoch 128/300\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.3403 - acc: 0.8538 - val_loss: 0.6260 - val_acc: 0.8022\n",
      "Epoch 129/300\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.3419 - acc: 0.8600 - val_loss: 0.6475 - val_acc: 0.8022\n",
      "Epoch 130/300\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.3301 - acc: 0.8712 - val_loss: 0.6818 - val_acc: 0.7912\n",
      "Epoch 131/300\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.3354 - acc: 0.8650 - val_loss: 0.6261 - val_acc: 0.8242\n",
      "Epoch 132/300\n",
      "800/800 [==============================] - 0s 99us/step - loss: 0.3383 - acc: 0.8625 - val_loss: 0.6425 - val_acc: 0.8022\n",
      "Epoch 133/300\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.3419 - acc: 0.8675 - val_loss: 0.6744 - val_acc: 0.8022\n",
      "Epoch 134/300\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.3421 - acc: 0.8488 - val_loss: 0.6586 - val_acc: 0.7912\n",
      "Epoch 135/300\n",
      "800/800 [==============================] - 0s 98us/step - loss: 0.3358 - acc: 0.8625 - val_loss: 0.6675 - val_acc: 0.8132\n",
      "Epoch 136/300\n",
      "800/800 [==============================] - 0s 98us/step - loss: 0.3351 - acc: 0.8738 - val_loss: 0.6458 - val_acc: 0.8462\n",
      "Epoch 137/300\n",
      "800/800 [==============================] - 0s 90us/step - loss: 0.3394 - acc: 0.8600 - val_loss: 0.6668 - val_acc: 0.7912\n",
      "Epoch 138/300\n",
      "800/800 [==============================] - 0s 99us/step - loss: 0.3425 - acc: 0.8675 - val_loss: 0.6576 - val_acc: 0.8022\n",
      "Epoch 139/300\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.3399 - acc: 0.8687 - val_loss: 0.6505 - val_acc: 0.8132\n",
      "Epoch 140/300\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.3314 - acc: 0.8675 - val_loss: 0.6537 - val_acc: 0.8132\n",
      "Epoch 141/300\n",
      "800/800 [==============================] - 0s 103us/step - loss: 0.3393 - acc: 0.8562 - val_loss: 0.7199 - val_acc: 0.7912\n",
      "Epoch 142/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.3426 - acc: 0.8538 - val_loss: 0.6618 - val_acc: 0.8352\n",
      "Epoch 143/300\n",
      "800/800 [==============================] - 0s 100us/step - loss: 0.3287 - acc: 0.8687 - val_loss: 0.6602 - val_acc: 0.8132\n",
      "Epoch 144/300\n",
      "800/800 [==============================] - 0s 96us/step - loss: 0.3254 - acc: 0.8675 - val_loss: 0.6547 - val_acc: 0.8022\n",
      "Epoch 145/300\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.3261 - acc: 0.8687 - val_loss: 0.6924 - val_acc: 0.8022\n",
      "Epoch 146/300\n",
      "800/800 [==============================] - 0s 109us/step - loss: 0.3263 - acc: 0.8738 - val_loss: 0.6734 - val_acc: 0.8571\n",
      "Epoch 147/300\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.3296 - acc: 0.8675 - val_loss: 0.6882 - val_acc: 0.8352\n",
      "Epoch 148/300\n",
      "800/800 [==============================] - 0s 104us/step - loss: 0.3326 - acc: 0.8663 - val_loss: 0.6872 - val_acc: 0.7912\n",
      "Epoch 149/300\n",
      "800/800 [==============================] - 0s 98us/step - loss: 0.3341 - acc: 0.8562 - val_loss: 0.6732 - val_acc: 0.8352\n",
      "Epoch 150/300\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.3345 - acc: 0.8638 - val_loss: 0.6866 - val_acc: 0.8022\n",
      "Epoch 151/300\n",
      "800/800 [==============================] - 0s 74us/step - loss: 0.3301 - acc: 0.8638 - val_loss: 0.7062 - val_acc: 0.8352\n",
      "Epoch 152/300\n",
      "800/800 [==============================] - 0s 85us/step - loss: 0.3356 - acc: 0.8562 - val_loss: 0.6852 - val_acc: 0.8022\n",
      "Epoch 153/300\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.3426 - acc: 0.8625 - val_loss: 0.6831 - val_acc: 0.8571\n",
      "Epoch 154/300\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.3429 - acc: 0.8625 - val_loss: 0.7696 - val_acc: 0.8022\n",
      "Epoch 155/300\n",
      "800/800 [==============================] - 0s 88us/step - loss: 0.3426 - acc: 0.8513 - val_loss: 0.7127 - val_acc: 0.7912\n",
      "Epoch 156/300\n",
      "800/800 [==============================] - 0s 88us/step - loss: 0.3418 - acc: 0.8588 - val_loss: 0.6634 - val_acc: 0.8242\n",
      "Epoch 157/300\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.3275 - acc: 0.8687 - val_loss: 0.6968 - val_acc: 0.7912\n",
      "Epoch 158/300\n",
      "800/800 [==============================] - 0s 81us/step - loss: 0.3214 - acc: 0.8700 - val_loss: 0.6934 - val_acc: 0.8242\n",
      "Epoch 159/300\n",
      "800/800 [==============================] - 0s 81us/step - loss: 0.3178 - acc: 0.8687 - val_loss: 0.6911 - val_acc: 0.8132\n",
      "Epoch 160/300\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.3251 - acc: 0.8725 - val_loss: 0.7103 - val_acc: 0.8022\n",
      "Epoch 161/300\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.3250 - acc: 0.8638 - val_loss: 0.7318 - val_acc: 0.8462\n",
      "Epoch 162/300\n",
      "800/800 [==============================] - 0s 86us/step - loss: 0.3489 - acc: 0.8463 - val_loss: 0.7647 - val_acc: 0.7912\n",
      "Epoch 163/300\n",
      "800/800 [==============================] - 0s 80us/step - loss: 0.3263 - acc: 0.8712 - val_loss: 0.6850 - val_acc: 0.8132\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 89us/step - loss: 0.3139 - acc: 0.8750 - val_loss: 0.7494 - val_acc: 0.7912\n",
      "Epoch 165/300\n",
      "800/800 [==============================] - 0s 85us/step - loss: 0.3233 - acc: 0.8700 - val_loss: 0.7050 - val_acc: 0.8462\n",
      "Epoch 166/300\n",
      "800/800 [==============================] - 0s 100us/step - loss: 0.3412 - acc: 0.8500 - val_loss: 0.6998 - val_acc: 0.8571\n",
      "Epoch 167/300\n",
      " 32/800 [>.............................] - ETA: 0s - loss: 0.2496 - acc: 0.8750"
     ]
    }
   ],
   "source": [
    "model64_5=reluModel64_5()\n",
    "model64_5.summary()\n",
    "hist2=model64_5.fit(x_train, y_train, epochs=300, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_score, test1_score=model64_3.evaluate(x_test, y_test)\n",
    "test2_score, test2_score=model64_5.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model64_5.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
